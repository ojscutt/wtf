"""
compile_from_dict.py

classes for reconstructing and running forward passes through a neural network
described by a dictionary (as generated by `tf_to_dict.py`). 

supports both NumPy and JAX backends for numerical computation.
"""

# =======================
# base class for building models from dict made with tf_to_dict
# =======================
class wtf_base:
    def __init__(self, wtf_dict, to_array_func, to_array_dtype):
        """
        base class that converts the model dictionary into a structured
        list of layers that can be executed sequentially.

        Parameters
        ----------
        wtf_dict : dict
            dictionary representing the network structure and parameters.
        to_array_func : callable
            backend-specific array creation function (e.g., np.array, jnp.array).
        to_array_dtype : dtype
            backend-specific floating point dtype (e.g., np.float32, jnp.float32).
        """
        
        self.wtf_dict = wtf_dict
        self.to_array_func = to_array_func
        self.to_array_dtype = to_array_dtype

        # count branches, if any      
        self.n_branches = len([key for key in list(self.wtf_dict['structure'].keys())[1:] if 'branch' in key])

        # build ordered lists of layers for stem or branches
        self.stem_layers, self.branches = self.build_model_from_dict(to_array_func, to_array_dtype)
        
    # -----------------------
    # convert list of layer names to flow of functions with appropriate backend
    # -----------------------
    def build_layers_from_dict(self, layer_list, layer_names, to_array_func, to_array_dtype):
        """
        convert named layers from the dictionary into tuples of weights, biases, and activation functions.
        """
        for layer_name in layer_names:
            layer = self.wtf_dict['layers'][layer_name]
            layer_type = layer['type']

            # dense: store weights, biases, and activation
            if layer_type == 'Dense':
                weights = to_array_func(layer['weights'], dtype=to_array_dtype)
                biases = to_array_func(layer['biases'], dtype=to_array_dtype)
                activation = layer['activation']
                
                layer_list.append(('Dense', weights, biases, activation))

            # input: store expected batch shape
            elif layer_type == 'InputLayer':
                
                layer_list.append(('InputLayer', layer['batch_shape']))

    # -----------------------
    # build full model structure from dict (stem and branches)
    # -----------------------
    def build_model_from_dict(self, to_array_func, to_array_dtype):
        """
        builds ordered layer lists for the model stem and each branch.
        """
        # stem (sequential pass)
        stem_layers = []
        stem_layer_names = list(self.wtf_dict['structure']['stem'].values())
        self.build_layers_from_dict(stem_layers, stem_layer_names, to_array_func, to_array_dtype)

        # branches (build separately)
        branches = []
        for branch_idx in range(self.n_branches):
            branch_layers = []
            branch_layer_names = list(self.wtf_dict['structure'][f'branch_{branch_idx}'].values())[1:] # skip branching node itself
            self.build_layers_from_dict(branch_layers, branch_layer_names, to_array_func, to_array_dtype)

            branches.append(branch_layers)

        return stem_layers, branches

# =======================
# NumPy backend for forward pass execution
# =======================
class numpy_compile(wtf_base):
    import numpy as np
    
    def __init__(self, wtf_dict):
        """
        build NumPy-compatible functions for forward passes.
        """
        super().__init__(wtf_dict, to_array_func=self.np.array, to_array_dtype=self.np.float32)

        # convert each layer into a callable function for stem + branches
        self.stem_functions = self.make_layer_functions(self.stem_layers)
        self.branches_functions = [self.make_layer_functions(branch_layers) for branch_layers in self.branches]

    # -----------------------
    # NumPy elu
    # -----------------------
    def elu(self, x):
        return self.np.where(x >= 0, x, self.np.exp(x)-1)

    # -----------------------
    # convert layers to callable functions
    # -----------------------
    def make_layer_functions(self, layers):
        """
        convert layer tuples into callable functions.
        """
        layer_functions = []
        for layer in layers:
            # dense: activation(x @ w + b)
            if layer[0] == 'Dense':
                _, weights, biases, activation = layer

                def layer_function(x, weights=weights, biases=biases, activation=activation):
                    x = x @ weights + biases
                    
                    if activation == 'elu':
                        x = self.elu(x)
                    return x
                    
                layer_functions.append(layer_function)

            # input: assert shape matches expected
            elif layer[0] == 'InputLayer':
                _, batch_shape = layer
                
                def layer_function(x, expected_batch_shape=batch_shape[1]):
                    if x.shape[1] != expected_batch_shape:
                        raise ValueError(f'wtf!\r \tinput shape = (n, {x.shape[1]}), but expected input layer batch_shape = (n, {expected_batch_shape})')
                    
                    return x
                    
                layer_functions.append(layer_function)
                
        return layer_functions   
        
    # -----------------------
    # forward pass helper functions
    # -----------------------
    def layer_functions_pass(self, layer_functions, x):
        for layer_function in layer_functions:
            x = layer_function(x)

        return x
        
    # stem: single sequential 
    def stem_pass(self, x):
        return self.layer_functions_pass(self.stem_functions, x)

    # branch: multiple sequential
    def branch_pass(self, stem_outputs):
        return [self.layer_functions_pass(branch_functions, stem_outputs) for branch_functions in self.branches_functions]

    # full forward pass with stem and branches
    def forward_pass(self, x):
        stem_output = self.stem_pass(x)
        return self.branch_pass(stem_output) if self.n_branches else stem_output

# =======================
# JAX backend for forward pass execution, JIT ready
# =======================
class jax_compile(wtf_base):
    import jax
    import jax.numpy as jnp
    
    def __init__(self, wtf_dict):
        """
        build JAX-compatible functions for forward passes.
        """
        super().__init__(wtf_dict, to_array_func=self.jnp.array, to_array_dtype=self.jnp.float32)

        # convert each layer into a callable function for stem + branches
        self.stem_functions = self.make_layer_functions(self.stem_layers)
        self.branches_functions = [self.make_layer_functions(branch_layers) for branch_layers in self.branches]

        # compiled version of forward_pass
        self.jit_forward_pass = self.jax.jit(self.forward_pass)

    # -----------------------
    # convert layers to callable functions
    # -----------------------
    def make_layer_functions(self, layers):
        """
        convert layer tuples into callable functions.
        """
        
        layer_functions = []
        for layer in layers:
            # dense: activation(x @ w + b)
            if layer[0] == 'Dense':
                _, weights, biases, activation = layer

                def layer_function(x, weights=weights, biases=biases, activation=activation):
                    x = x @ weights + biases
                    
                    if activation == 'elu':
                        x = self.jax.nn.elu(x) # inline JAX elu
                        
                    return x
                    
                layer_functions.append(layer_function)

            # input: no batch assert! run NumPy backend first
            elif layer[0] == 'InputLayer':
                _, batch_shape = layer
                # no assert here, JIT doesn't like it
                def layer_function(x):
                    return x
                    
                layer_functions.append(layer_function)
                
        return layer_functions   

    # -----------------------
    # forward pass helper functions
    # -----------------------
    def layer_functions_pass(self, layer_functions, x):
        for layer_function in layer_functions:
            x = layer_function(x)

        return x

    # stem: single sequential     
    def stem_pass(self, x):
        return self.layer_functions_pass(self.stem_functions, x)

    # branch: multiple sequential
    def branch_pass(self, stem_outputs):
        return [self.layer_functions_pass(branch_functions, stem_outputs) for branch_functions in self.branches_functions]
        
    # full forward pass with stem and branches, compiled version in jit_forward_pass
    def forward_pass(self, x):
        stem_output = self.stem_pass(x)
        return self.branch_pass(stem_output) if self.n_branches else stem_output
    

        

        
                
        



            
            
            
        











        

