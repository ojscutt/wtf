{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8eee879-6176-4a53-84a7-d9aec4ca6128",
   "metadata": {},
   "source": [
    "# compile_from_dict.ipynb\n",
    "***example of compiling a tensorflow-free model from a dictionary saved with tf_to_dict, using either numpy or jax***\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94bcb43-1903-43a7-bf12-9a0cb3317c59",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9308dcd-41e4-45f4-aa77-ccb4d6da4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from compile_from_dict import numpy_compile, jax_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2512ffc-bcab-4929-8eec-704e4d85fac8",
   "metadata": {},
   "source": [
    "## load in dictionary made using `tf_to_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62565d-fb96-4e98-b525-9dbf0c3f5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pitchfork'\n",
    "with open(f'models/{model_name}.json', 'r') as fp:\n",
    "    model_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e91841-3c6b-4139-9472-8cfbd26257c4",
   "metadata": {},
   "source": [
    "## numpy_compile\n",
    "let's take a look at how we use numpy_compile to interpret the `model_dict` dictionary made using `tf_to_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc852e-c483-4823-b8b1-8f2c9dceac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_model = numpy_compile(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4f0a6-15df-453b-8b48-862950f2ed63",
   "metadata": {},
   "source": [
    "done! `numpy_model` is now a an object containing a flow of functions written purely in numpy which represent a forwards pass through the network\n",
    "\n",
    "the example network, `pitchfork`, takes a set of 5 inputs and predicts 3 outputs on one branch and 38 on the other - let's define a test point of arbitrary values, and check that this is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091867a-b891-467c-81e1-dffcecd5a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_single_point = np.array([[0.5,0.5,0.5,0.5,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae2d13-a1cc-4e4b-8e70-8c95a86f3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_model.forward_pass(numpy_single_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aea38d-b720-422b-8260-c781eca04686",
   "metadata": {},
   "source": [
    "nice! that seemed fast, but let's time it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbb7d1-5345-49d8-aa15-52d69b26fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "numpy_model.forward_pass(numpy_single_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9e385-8042-48a6-87dd-9f2f2c33625d",
   "metadata": {},
   "source": [
    "we typically want neural networks to predict on huge batches at once rather than just single points, though.\n",
    "\n",
    "let's check that this functionality isn't lost when compiling from our dict, and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed0769-3f91-42f7-a81a-0c565e68c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_many_points = np.full((100000,5), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fceb9-7a62-4c53-a2e8-7d7e679e0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "numpy_model.forward_pass(numpy_many_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa880e2-569e-44ab-83d6-b5b7ebe33bf3",
   "metadata": {},
   "source": [
    "also pretty fast!\n",
    "\n",
    "however, we can definitely make this faster by using jax (which utilises the GPU where possible), and even faster if we then JIT compile the jax predict function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a85850-f96d-4e33-98d6-ed310be823c5",
   "metadata": {},
   "source": [
    "## jax_compile\n",
    "this time, let's try the same model but compiled entirely in jax - same process as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e93cd3-8762-45b3-98c7-0928165a170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_model = jax_compile(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e811a00-de11-4fc8-bef5-c10791ce47bb",
   "metadata": {},
   "source": [
    "the `jax_model` object is written entirely in jax.numpy - so we want to be careful that we're only passing in jax objects otherwise we might be losing valuable time!\n",
    "\n",
    "lets define some test points like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15fb01-a326-4a37-93df-0d7ec9a81a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_single_point = jnp.array([[0.5,0.5,0.5,0.5,0.5]])\n",
    "jax_many_points = jnp.full((100000,5), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1483520-631f-4110-9e89-19d77aea87fc",
   "metadata": {},
   "source": [
    "and then we can perform a forward pass and time for one point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a4872-c723-4fd3-b9f8-94758863f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jax_model.forward_pass(jax_single_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290b21d-cb5b-4088-b394-417da27ba6b0",
   "metadata": {},
   "source": [
    "or for a batch of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56bcf0-863c-4630-921b-559e9457dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jax_model.forward_pass(jax_many_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1b2a1-1c6c-4e1c-b642-fb7cf4f86f04",
   "metadata": {},
   "source": [
    "this may be faster than the numpy version or not depending on your machine.\n",
    "\n",
    "one way that we can certainly speed this up is by jit compiling the flow of functions in jax_model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea94a03-cb1d-4ce6-8188-3eaf834ab21d",
   "metadata": {},
   "source": [
    "## jax_compile.jit_forward_pass\n",
    "as before, we compile our model from the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210f77c-aaff-4432-9b40-12bbc44c1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_model = jax_compile(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d2955-83c8-411a-a715-5901eb1315a5",
   "metadata": {},
   "source": [
    "and define some jax friendly test points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb127f-92a0-4bec-a9a0-0016aeea29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_single_point = jnp.array([[0.5,0.5,0.5,0.5,0.5]])\n",
    "jax_many_points = jnp.full((100000,5), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0289fe-6df2-4639-adf0-ab6794920322",
   "metadata": {},
   "source": [
    "now let's try using the jit compiled version of `forward_pass`, and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7095d-bd42-4a97-b120-2feb13415ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jax_model.jit_forward_pass(jax_single_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575b1d2-c2ad-4c46-ad55-f63d94752585",
   "metadata": {},
   "source": [
    "oh dear! this is (probably) slower than your `numpy_model.forward_pass(np_single_point)` or `jax_model.forwad_pass(jax_single_point)` cells!\n",
    "\n",
    "actually, this is entirely expected because of the way JIT compilation works - the flow of functions is compiled each time for a **specific input shape**, and there is a small overhead associated with JIT compilation.\n",
    "\n",
    "***this is an important point - JIT compiling might not help us (and in fact slow things down) if our batch sizes change dynamically!***\n",
    "\n",
    "let's see whether we do better now that we've compiled the single point pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35936e-974a-4b6d-8b7e-f42e1199c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jax_model.jit_forward_pass(jax_single_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be8f6e-2584-48f9-92ec-a535cda10937",
   "metadata": {},
   "source": [
    "nice!\n",
    "\n",
    "what about for the batch of many points? we'll run and time one cell to compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc0c49-e63e-4e40-9275-df1024fa627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jax_model.jit_forward_pass(jax_many_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329ba87-1e5e-4909-a382-ae5cbe030017",
   "metadata": {},
   "source": [
    "and then again to time the compiled version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df165832-1fa7-46a2-a93b-3a87ce7a45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jax_model.jit_forward_pass(jax_many_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07400296-5b79-41ad-a428-c45ccc46d984",
   "metadata": {},
   "source": [
    "this should be much faster!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
